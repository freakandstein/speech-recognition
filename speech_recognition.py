import collections
import numpy as np
import torch
import whisper
import pyaudio
import pyautogui

import re
from difflib import get_close_matches
from silero_vad import load_silero_vad


# â”€â”€ Silero VAD chunk size harus 512 samples pada 16kHz â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAMPLE_RATE = 16000
CHUNK_SAMPLES = 512  # ~32ms per chunk, required by Silero VAD
VOICE_COMMANDS = {
    "first camera": ["command", "3"],
    "second camera": ["command", "4"],
    "record": ["command", "r"],
    # Add more mappings here
}

def normalize_text(text):
    # Lowercase, remove punctuation, collapse whitespace
    text = text.lower()
    text = re.sub(r'[^a-z0-9 ]+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def run_shortcut(keys):
    pyautogui.hotkey(*keys)

def load_whisper_model(model_name, device):
    """Load Whisper model, handle sparse tensor untuk MPS."""
    print(f"ğŸ“¦ Loading Whisper model '{model_name}'...")
    model = whisper.load_model(model_name, device="cpu")

    if device == "mps":
        for name, buf in list(model.named_buffers()):
            if buf.is_sparse:
                parts = name.split(".")
                parent = model
                for part in parts[:-1]:
                    parent = getattr(parent, part)
                parent.register_buffer(parts[-1], buf.to_dense())
        model = model.to(device)

    print(f"âœ… Model loaded on {device}\n")
    return model


def list_input_devices():
    """Tampilkan semua input device yang tersedia."""
    pa = pyaudio.PyAudio()
    devices = []
    for i in range(pa.get_device_count()):
        info = pa.get_device_info_by_index(i)
        if info["maxInputChannels"] > 0:
            devices.append((i, info["name"], int(info["maxInputChannels"])))
    pa.terminate()
    return devices


def select_input_device():
    """Interaktif: tampilkan daftar device dan minta user pilih."""
    devices = list_input_devices()
    print("\nğŸ™ï¸  Input devices tersedia:")
    for idx, name, ch in devices:
        print(f"  [{idx}] {name} (ch: {ch})")

    default_idx = None
    for idx, name, ch in devices:
        if "macbook" in name.lower() or "built-in" in name.lower():
            default_idx = idx
            break
    if default_idx is None:
        default_idx = devices[0][0]

    try:
        choice = input(f"\nPilih device index (Enter = [{default_idx}] default): ").strip()
        selected = int(choice) if choice else default_idx
        # Validasi
        valid_ids = [d[0] for d in devices]
        if selected not in valid_ids:
            print(f"âš ï¸  Index tidak valid, pakai default [{default_idx}]")
            selected = default_idx
    except ValueError:
        selected = default_idx

    chosen = next((d for d in devices if d[0] == selected), None)
    print(f"âœ… Menggunakan: [{chosen[0]}] {chosen[1]}\n")
    return selected, chosen[2]  # return (device_index, channels)


def get_audio_rms(chunk_bytes):
    """Hitung RMS dari raw audio bytes."""
    audio = np.frombuffer(chunk_bytes, dtype=np.int16).astype(np.float32)
    return np.sqrt(np.mean(audio ** 2))


def bytes_to_tensor(chunk_bytes):
    """Konversi raw bytes ke float32 tensor untuk Silero VAD."""
    audio = np.frombuffer(chunk_bytes, dtype=np.int16).astype(np.float32) / 32768.0
    return torch.from_numpy(audio)


def record_with_silero(silero_model, device_index=None, sample_rate=SAMPLE_RATE):
    """
    Rekam audio menggunakan Silero VAD (neural network) untuk deteksi
    suara manusia yang akurat.

    Silero mengembalikan confidence score 0.0â€“1.0:
      > 0.5  â†’ kemungkinan besar suara manusia
      < 0.35 â†’ bukan suara manusia / silence
    """
    pa = pyaudio.PyAudio()
    stream = pa.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=sample_rate,
        input=True,
        input_device_index=device_index,
        frames_per_buffer=CHUNK_SAMPLES,
    )

    print("\nğŸ‘‚ Mendengarkan... (berbicara untuk mulai merekam)")
    print("=" * 50)

    # Ring buffer: simpan ~20 chunk terakhir (~640ms)
    ring_buffer = collections.deque(maxlen=20)
    triggered = False
    voiced_frames = []
    silent_chunks = 0

    # 1200ms / 32ms per chunk â‰ˆ 37 chunks diam sebelum berhenti rekam
    max_silent_chunks = 37

    SPEECH_THRESHOLD = 0.5    # di atas ini â†’ suara manusia
    SILENCE_THRESHOLD = 0.35  # di bawah ini â†’ silence / non-human

    # Flush stream dulu
    for _ in range(10):
        stream.read(CHUNK_SAMPLES, exception_on_overflow=False)

    # Kalibrasi noise floor untuk energy gate
    print("ğŸ”§ Kalibrasi noise floor... (diam sebentar)")
    noise_rms_samples = []
    for _ in range(50):
        cal_chunk = stream.read(CHUNK_SAMPLES, exception_on_overflow=False)
        noise_rms_samples.append(get_audio_rms(cal_chunk))
    # Buang outlier dulu: hapus sampel yang > median Ã— 3
    # (jika ada suara keras saat kalibrasi, sampelnya dibuang sebelum hitung threshold)
    arr = np.array(noise_rms_samples)
    median = np.median(arr)
    clean = arr[arr <= median * 3]  # hanya pakai sampel yang wajar
    if len(clean) < 5:
        clean = arr  # fallback jika terlalu banyak yang dibuang
    noise_floor = np.percentile(clean, 75)  # P75 dari sampel bersih
    energy_threshold = max(200, noise_floor * 1.5)
    print(f"ğŸ“Š Noise median: {median:.1f} | Clean P75: {noise_floor:.1f} | Energy gate: {energy_threshold:.1f}")
    print(f"   ({len(arr) - len(clean)} sampel outlier dibuang dari {len(arr)} total)\n")

    try:
        while True:
            chunk_bytes = stream.read(CHUNK_SAMPLES, exception_on_overflow=False)

            if len(chunk_bytes) != CHUNK_SAMPLES * 2:
                continue

            # â”€â”€ Silero VAD: confidence score 0.0â€“1.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            with torch.no_grad():
                tensor = bytes_to_tensor(chunk_bytes)
                confidence = silero_model(tensor, sample_rate).item()

            # â”€â”€ Energy gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            chunk_rms = get_audio_rms(chunk_bytes)
            energy_ok = chunk_rms > energy_threshold

            # Valid speech = Silero yakin DAN energy cukup
            is_speech = (confidence > SPEECH_THRESHOLD) and energy_ok

            # â”€â”€ Debug real-time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            status = "ğŸ—£ " if is_speech else "   "
            print(
                f"  {status} Conf: {confidence:.2f} | RMS: {chunk_rms:6.1f} | "
                f"{'RECORDING' if triggered else f'buf: {sum(1 for _,v in ring_buffer if v)}/{ring_buffer.maxlen}'}",
                end="\r"
            )

            if not triggered:
                ring_buffer.append((chunk_bytes, is_speech))
                num_voiced = sum(1 for _, v in ring_buffer if v)

                # Trigger jika 75% dari ring buffer adalah speech
                if num_voiced >= 0.75 * ring_buffer.maxlen:
                    triggered = True
                    print(f"\nğŸ¤ Suara manusia terdeteksi! (conf: {confidence:.2f}) Merekam...")
                    voiced_frames.extend([c for c, _ in ring_buffer])
                    ring_buffer.clear()
                    silent_chunks = 0
            else:
                voiced_frames.append(chunk_bytes)

                is_silent = (confidence < SILENCE_THRESHOLD) or not energy_ok
                if is_silent:
                    silent_chunks += 1
                else:
                    silent_chunks = 0

                if silent_chunks > max_silent_chunks:
                    print("\nğŸ”‡ Selesai berbicara, memproses...")
                    break

    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()

    return b"".join(voiced_frames)


def main():
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  Using device: {device}")

    # Pilih input device
    device_index, _ = select_input_device()

    # Load Silero VAD â€” neural network, jauh lebih akurat dari WebRTC VAD
    print("ğŸ“¦ Loading Silero VAD...")
    silero_model = load_silero_vad()
    silero_model.eval()
    print("âœ… Silero VAD loaded\n")

    # Load Whisper
    whisper_model = load_whisper_model("small", device)

    print("ğŸ” Tekan Ctrl+C untuk berhenti\n")
    try:
        while True:
            audio_bytes = record_with_silero(silero_model, device_index=device_index)

            duration_s = len(audio_bytes) / (SAMPLE_RATE * 2)
            overall_rms = np.sqrt(
                np.mean(np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) ** 2)
            )

            # Filter: terlalu pendek
            if duration_s < 0.5:
                print("â­ï¸  SKIP: terlalu pendek\n")
                continue

            # Filter: energi terlalu rendah
            if overall_rms < 300:
                print("â­ï¸  SKIP: energi terlalu rendah\n")
                continue

            print("ğŸ” Memproses...")
            audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0

            result = whisper_model.transcribe(
                audio_np,
                language="en",
                fp16=False,
                no_speech_threshold=0.6,
                logprob_threshold=-1.0,
                compression_ratio_threshold=2.4,
                condition_on_previous_text=False,
            )
            text = result["text"].strip()

            print("=" * 50)
            if text and text.replace(".", "").replace(",", "").replace(" ", ""):
                print(f"ğŸ“ {text}")
                # Normalize and fuzzy match
                cmd = normalize_text(text)
                print(f"ğŸ”— Looking for shortcut for: '{cmd}'")
                match = None
                if cmd in VOICE_COMMANDS:
                    match = cmd
                else:
                    # Fuzzy match: allow minor typo or punctuation difference
                    close = get_close_matches(cmd, VOICE_COMMANDS.keys(), n=1, cutoff=0.8)
                    if close:
                        match = close[0]
                if match:
                    print(f"ğŸ”— Executing shortcut for: {match}")
                    run_shortcut(VOICE_COMMANDS[match])
            else:
                print("ğŸ“ (tidak terdeteksi ucapan)")
            print("=" * 50)
            print()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Selesai!")


if __name__ == "__main__":
    main()